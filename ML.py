# -*- coding: utf-8 -*-
"""OHKL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x3kiKnxF_OqTYzMjhUTMbjeKt6-WUoqo

# Machine learning model to predict chl-a concentration in the sabah coast of Malaysia, Kota Kinabalu (LSTM model) (2013 to 2020 data)
"""

# give access to drive
from google.colab import drive
drive.mount('/content/drive')

# importing the library needed

from tensorflow import keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Conv1D, LSTM, MaxPool1D
from tensorflow.keras.callbacks import ModelCheckpoint

import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler

# read the data 
df = pd.read_csv("/content/drive/MyDrive/Ocean_Hackathon/chla_by_sites.csv")
df

# choose only one site for machine learning model 
site1 = df.drop(['lat_site_1','lon_site_1','lat_site_2','lon_site_2','lat_site_3','lon_site_3','lat_site_4','lon_site_4','CHL_2','CHL_3','CHL_4'],axis=1)

site1

site1 = site1.set_index('time')

# choose data from 2018-01-01 onwards for training 
# df = df[df['date']>"2017-12-31"].set_index('date')

factor = site1.CHL_1.max()
factor

# normalise chl data, sst and par
site1['CHL_1']=site1['CHL_1']/factor
#df['analysed_sst']=df['analysed_sst']/df.analysed_sst.max()
#df['PAR_mean']=df['PAR_mean']/df.PAR_mean.max()

site1.to_numpy()

# from the dataset, we then now extract the neccessary data into the input and output
# since we plan to use a LSTM model to learn the time-series pattern, we decide to use a window size of 7
# hence, we define a function to achieve that

def get_X_Y(df,window=6):
    df_numpy = df.to_numpy()
    x = []
    y = []
    
    for i in range(len(df_numpy)-window):
        row = df_numpy[i:i+window]
        x.append(row)
        
        label = df_numpy[i+window]
        y.append(label)
        
    return np.array(x),np.array(y)

x,y = get_X_Y(site1)

# examine the shape of input and output
x.shape, y.shape

# reshaping the y data input shape
y = y.reshape(2916)

2916*10/100

x_train,y_train = x[0:2332], y[0:2332]  #2013-2018. #80%
x_val,y_val=x[2332:2623], y[2332:2623]  # 2019.    #10%
x_test,y_test=x[2623:2915], y[2623:2915] # 2020     #10%

x_train.shape,y_train.shape,x_val.shape,y_val.shape,x_test.shape,y_test.shape

from tensorflow.keras.optimizers import Adam

opt1 = Adam(learning_rate=0.01)

# building the model

model = Sequential()
#model.add(MaxPool1D(pool_size=2,input_shape=(6,3)))
#model.add(Conv1D(filters=64, kernel_size=3,input_shape=(6,3),activation='relu'))
#model.add(Dropout(0.2))
model.add(LSTM(64, activation='tanh',return_sequences=True,input_shape=(6,1))) #return_sequence = False
model.add(Dropout(0.2))
model.add(LSTM(32, activation='tanh'))
#model.add(LSTM(64, activation='tanh'))
#model.add(Dense(60,activation='relu',input_shape=(6,1)))
model.add(Dropout(0.2))
#model.add(Dense(32,activation='relu'))
model.add(Dense(30,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.summary()

model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mean_absolute_error'])

model.fit(x_train,y_train,validation_data=(x_val,y_val),verbose=1,epochs=30,batch_size=6)

b = model.predict(x_test)

train_result=pd.DataFrame({'predicted':b.flatten(),'actual':y_test.flatten()})
train_result

factor

# factor back the original concentration of chlorophyll a
train_result['predicted']=train_result['predicted']*factor
train_result['actual']=train_result['actual']*factor
train_result

plt.plot(train_result.index,train_result.predicted)
plt.plot(train_result.index,train_result.actual)
plt.xlabel('Day')
plt.ylabel('Chl-a (mg/m$-3$)')
plt.savefig('/content/drive/MyDrive/Ocean_Hackathon/well_trainned.jpg')

import scipy.stats as sts

r,p=sts.pearsonr(train_result['predicted'],train_result['actual'])

# test the r value of predicted and actual data
r

"""# Using other data with different time range to test the built model (November to December 2022 data)"""

# saving the model for future prediciting purpose 
model.save('/content/drive/MyDrive/Ocean_Hackathon/prediciting_model.h5')

# load the saved model into memomry 
model = load_model("/content/drive/MyDrive/Ocean_Hackathon/for dashboard/prediciting_model.h5")

# import data for forecasting purpose
data_forcasting = pd.read_csv('/content/drive/MyDrive/Ocean_Hackathon/forcasting.csv')

data_forcasting = data_forcasting.drop('analysed_sst',axis=1).set_index('time')

# the scale factor used in the model, going to apply in the predicting data 
factor

# scale the data
data_forcasting['CHL']=data_forcasting['CHL']/factor

# divide the data into test and actual set 
x1,y1=get_X_Y(data_forcasting)

# check the shape of the data, which have to fit in the size of model which have trained 
x1.shape,y1.shape

# predicting the future value 
c = model.predict(x1)

# compared the predicting with actual 
train_result_train=pd.DataFrame({'predicted':c.flatten(),'actual':y1.flatten()})
train_result_train

# get the r value 
r1,p1=sts.pearsonr(train_result_train['predicted'],train_result_train['actual'])

r1

# visualise predicting value 
plt.plot(train_result_train.index,train_result_train.predicted)
plt.plot(train_result_train.index,train_result_train.actual)